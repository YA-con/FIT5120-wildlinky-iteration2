# -*- coding: utf-8 -*-
"""Iteration 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18KTSbHpTT2iVwe2uVLOq2qryDhvVcCln
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Ankit**

### **Import data**
"""

# LEADBEATERS POSSUM AND HELMETED HONEYEATER
import pandas as pd

df_possum = pd.read_excel("/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/Combined_species/leadbeaters_possum_all.xlsx")
df_heater = pd.read_excel("/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/Combined_species/Helmeted_Honeyeater_All.xlsx")

#df_heater.columns
df_possum.columns

"""### **1. Extract year from datetime**"""

import pandas as pd

# Step 1: Make sure the column is in datetime format
df_possum['Survey Start Date'] = pd.to_datetime(df_possum['Survey Start Date'], errors='coerce')
df_heater['Survey Start Date'] = pd.to_datetime(df_heater['Survey Start Date'], errors='coerce')
# Step 2: Extract the year
df_possum['Start Year'] = df_possum['Survey Start Date'].dt.year
df_heater['Start Year'] = df_heater['Survey Start Date'].dt.year

df_heater['Start Year'].isna().sum()
min_year = df_heater['Start Year'].min()
max_year = df_heater['Start Year'].max()

print(f"Year range: {min_year} to {max_year}")

df_possum['Start Year'].isna().sum()
min_year = df_possum['Start Year'].min()
max_year = df_possum['Start Year'].max()

print(f"Year range: {min_year} to {max_year}")

# year_counts = df_possum['Start Year'].value_counts().sort_index()
# print(year_counts)
year_counts = df_possum[df_possum['Start Year'] > 2000]['Start Year'].value_counts().sort_index()
print(year_counts)

year_counts = df_heater[df_heater['Start Year'] > 2000]['Start Year'].value_counts().sort_index()
print(year_counts)

print(df_heater.shape)
print(df_possum.shape)

"""### **2.Filter year 2010 to 2020 and handle missing value**"""

# Step 1: Drop rows where Start Year is missing
df_pos_filtered = df_possum.dropna(subset=['Start Year'])
df_heat_filtered = df_heater.dropna(subset=['Start Year'])
# Step 2: Filter for years between 2010 and 2020 (inclusive)
df_pos_filtered = df_pos_filtered[
    (df_pos_filtered['Start Year'] >= 2000) & (df_pos_filtered['Start Year'] <= 2020)
]

df_heat_filtered = df_heat_filtered[
    (df_heat_filtered['Start Year'] >= 2000) & (df_heat_filtered['Start Year'] <= 2020)
]

df_pos_filtered.shape

df_heat_filtered.shape

"""### **3. Add speciesid**"""

df_pos_filtered['species_id'] = 2
df_heat_filtered['species_id'] = 1

df_pos_filtered.columns

"""### **4. Delete unnessary columns**"""

print(df_pos_filtered.columns.tolist())

for col in df_pos_filtered.columns:
    print(repr(col))

# List only the columns you want
columns_to_keep = [
    'Common Name',
    'Site Name',
    'Site Location Desc',
    'Latitude GDA94',
    'Longitude GDA94',
    'Total Count',
    'Start Year',
    'species_id'
]

# Create a new DataFrame with only those columns
df_cleaned_pos = df_pos_filtered[columns_to_keep]
df_cleaned_heat = df_heat_filtered[columns_to_keep]

"""### **5. Change column name**"""

df_cleaned_heat.columns

df_cleaned_heat = df_cleaned_heat.rename(columns={
    'Common Name': 'name',
    'Site Name': 'site_name',
    'Site Location Desc': 'site_desc',
    'Latitude GDA94': 'lat',
    'Longitude GDA94': 'long',
    'Total Count': 'count',
    'Start Year': 'year',
    'species_id': 'species_id'
})

df_cleaned_pos = df_cleaned_pos.rename(columns={
    'Common Name': 'name',
    'Site Name': 'site_name',
    'Site Location Desc': 'site_desc',
    'Latitude GDA94': 'lat',
    'Longitude GDA94': 'long',
    'Total Count': 'count',
    'Start Year': 'year',
    'species_id': 'species_id'
})

unique_years = df_cleaned_pos['year'].dropna().unique()
print('Possum: ',sorted(unique_years))
unique_years = df_cleaned_heat['year'].dropna().unique()
print('Heater: ',sorted(unique_years))

import numpy as np

# Define conditions
conditions = [
    (df_cleaned_heat['year'] >= 2000) & (df_cleaned_heat['year'] <= 2010),
    (df_cleaned_heat['year'] > 2010) & (df_cleaned_heat['year'] <= 2020)
]

# Corresponding period labels
choices = ['2000-2010', '2011-2020']

# Create the new column
df_cleaned_heat['period'] = np.select(conditions, choices, default='Other')

# Define conditions
conditions_pos = [
    (df_cleaned_pos['year'] >= 2000) & (df_cleaned_pos['year'] <= 2010),
    (df_cleaned_pos['year'] > 2010) & (df_cleaned_pos['year'] <= 2020)
]

# Create the new column
df_cleaned_pos['period'] = np.select(conditions_pos, choices, default='Other')

na_row_count = df_cleaned_heat.isna().any(axis=1).sum()
print(f"Number of rows with at least one NaN: {na_row_count}")

# Desired column order
column_order = [
    'species_id', 'name', 'lat', 'long', 'year', 'period',
    'count', 'site_name', 'site_desc'
]

# Reorder the DataFrame
df_cleaned_heat = df_cleaned_heat[column_order]
df_cleaned_pos = df_cleaned_pos[column_order]

# df_cleaned_pos.shape
df_cleaned_heat.shape

# Concatenate the two DataFrames
df_combined = pd.concat([df_cleaned_heat, df_cleaned_pos], ignore_index=True)

df_combined.columns

print(2165+322)

df_combined.shape

df_combined = df_combined[
    ['species_id', 'name', 'lat', 'long', 'year', 'period', 'count','site_name', 'site_desc']
]

df_combined



"""## **Ngan**"""

import pandas as pd

df3 = pd.read_excel('/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/Combined_species/greater_glider_all.xlsx')
print(df3.shape)
print(df3.columns)

file_path = '/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/Combined_species/brush_tailed_rock_wallaby_All.xlsx'
df4 = pd.read_excel(file_path)
print(df4.shape)
print(df4.columns)

file_path = '/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/Combined_species/Spot-tailed_Quoll.csv'
df5 = pd.read_csv(file_path)
print(df5.shape)
print(df5.columns)
df5['Common Name'] = "Spot-tailed Quoll"
print(df5.head())

"""### **1. Extract year from startdate**"""

df3['Survey Start Date'] = pd.to_datetime(df3['Survey Start Date'], errors='coerce')
df3['Survey Start Date'] = df3['Survey Start Date'].dt.year
df4['Survey Start Date'] = pd.to_datetime(df4['Survey Start Date'], errors='coerce')
df4['Survey Start Date'] = df4['Survey Start Date'].dt.year
df5['Survey Start Date'] = pd.to_datetime(df5['Survey Start Date'], errors='coerce')
df5['Survey Start Date'] = df5['Survey Start Date'].dt.year

"""### **2.Filter year 2000 to 2020 and handle missing value**
Notes that: should filter the reliability
"""

filtered_df3 = df3[(df3['Survey Start Date'] >= 2000) & (df3['Survey Start Date'] <= 2020)]
filtered_df4 = df4[(df4['Survey Start Date'] >= 2000) & (df4['Survey Start Date'] <= 2020)]
filtered_df5 = df5[(df5['Survey Start Date'] >= 2000) & (df5['Survey Start Date'] <= 2020)]

"""### **3. Add species_id, period**"""

# NGAN #
filtered_df3.loc[:, 'species_id'] = 3
filtered_df4.loc[:, 'species_id'] = 4
filtered_df5.loc[:, 'species_id'] = 5

filtered_df3['Survey Year'] = filtered_df3['Survey Start Date'].astype(int)
def assign_period_fixed(year):
    if 2000 <= year <= 2010:
        return "2000-2010"
    elif 2011 <= year <= 2020:
        return "2011-2020"
    else:
        return "Other"

filtered_df3['period'] = filtered_df3['Survey Year'].apply(assign_period_fixed)

filtered_df4['Survey Year'] = filtered_df4['Survey Start Date'].astype(int)
def assign_period_fixed(year):
    if 2000 <= year <= 2010:
        return "2000-2010"
    elif 2011 <= year <= 2020:
        return "2011-2020"
    else:
        return "Other"

filtered_df4['period'] = filtered_df4['Survey Year'].apply(assign_period_fixed)

filtered_df5['Survey Year'] = filtered_df5['Survey Start Date'].astype(int)
def assign_period_fixed(year):
    if 2000 <= year <= 2010:
        return "2000-2010"
    elif 2011 <= year <= 2020:
        return "2011-2020"
    else:
        return "Other"

filtered_df5['period'] = filtered_df5['Survey Year'].apply(assign_period_fixed)

filtered_df5

"""### **4. Delete unnessary columns**
keep these column species_id, name, lat, long, year, period, count, region, site_name, site_desc
"""

df3_final = filtered_df3[[
    'species_id', 'Common Name', 'Latitude GDA94', 'Longitude GDA94',
    'Survey Year', 'period', 'Total Count', 'Site Name', 'Site Location Desc'
]]
df4_final = filtered_df4[[
    'species_id', 'Common Name', 'Latitude GDA94', 'Longitude GDA94',
    'Survey Year', 'period', 'Total Count', 'Site Name', 'Site Location Desc'
]]
df5_final = filtered_df5[[
    'species_id', 'Common Name', 'Latitude GDA94', 'Longitude GDA94',
    'Survey Year', 'period', 'Total Count', 'Site Name', 'Site Location Desc'
]]

"""### **5. Change column name**"""

df3_final = df3_final.rename(columns={
    'Common Name': 'name',
    'Latitude GDA94': 'lat',
    'Longitude GDA94': 'long',
    'Survey Year': 'year',
    'Total Count': 'count',
    'Site Name': 'site_name',
    'Site Location Desc': 'site_desc'
})
df3_final

df4_final = df4_final.rename(columns={
    'Common Name': 'name',
    'Latitude GDA94': 'lat',
    'Longitude GDA94': 'long',
    'Survey Year': 'year',
    'Total Count': 'count',
    'Site Name': 'site_name',
    'Site Location Desc': 'site_desc'
})
df4_final

df5_final = df5_final.rename(columns={
    'Common Name': 'name',
    'Latitude GDA94': 'lat',
    'Longitude GDA94': 'long',
    'Survey Year': 'year',
    'Total Count': 'count',
    'Site Name': 'site_name',
    'Site Location Desc': 'site_desc'
})
df5_final

"""## **Combine all files and export as csv**
Combine dataset Ankit (df_combined) and Ngan (df3_final, df4_final, d5_final)
"""

combined_all = pd.concat([df_combined, df3_final, df4_final, df5_final], ignore_index=True)

combined_all['count'] = combined_all['count'].replace([' ', 'Acc'], 1)

# Convert all values in 'count' to numeric, handling errors
combined_all['count'] = pd.to_numeric(combined_all['count'], errors='coerce')

# Fill NaN values with a suitable integer (e.g., 0 or 1)
combined_all['count'] = combined_all['count'].fillna(1).astype(int)

# Display the unique names to confirm the replacement
combined_all['name'].unique()

csv_output_path = "/content/drive/MyDrive/FIT5120/Main_project/Datascience/Dataset/Victoria_species/vic_species_location.csv"
combined_all.to_csv(csv_output_path, index=False)

csv_output_path